{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 吳君慧\n",
    "\n",
    "Student ID: 113065539\n",
    "\n",
    "GitHub ID: Iris6636\n",
    "\n",
    "Kaggle name: Rondnoir\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "![pic0](img/pic0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY**\n",
    "\n",
    "---\n",
    "\n",
    "### **預處理**\n",
    "\n",
    "考量到推特文章的特性，例如包含 URL、@tag 用戶等，為避免這些元素對情緒分類的影響，設計了相應的前處理流程，目的是提供更乾淨且不易誤導的數據給 BERT 模型進行後續分析。\n",
    "\n",
    "---\n",
    "\n",
    "### **資料基本分析**\n",
    "\n",
    "1. **情緒類別占比**：\n",
    "   - 各情緒類別分佈差異較大，其中 `joy` 類別占比最高，明顯多於其他類別。\n",
    "   \n",
    "2. **數據完整性**：\n",
    "   - 經過檢查，`train set` 中沒有情緒標籤 (`emotion attribute`) 的空值情況。\n",
    "\n",
    "---\n",
    "\n",
    "### **模型選用**\n",
    "\n",
    "為了更好地捕捉推文中的情緒特徵，選用了基於 Transformer 架構的模型：\n",
    "- **模型選擇原因**：\n",
    "  - 使用 `distilbert-base-uncased` 模型，這是一個基於 BERT (`bert-base-uncased`) 的蒸餾版本，具備以下優勢：\n",
    "    1. 參數量僅為原模型的 60%，輕量化且高效。\n",
    "    2. 訓練速度與推論速度提升約 40%，非常適合硬體資源有限的環境。\n",
    "\n",
    "---\n",
    "\n",
    "### **實驗**\n",
    "\n",
    "#### **實驗 1**\n",
    "\n",
    "1. 考量到各類別訓練資料不平均的狀況，採用 **undersampling**。\n",
    "2. 藉由降低數據量，希望模型訓練時間縮短，能盡快得到一個成績作為 baseline。\n",
    "3. 訓練參數：\n",
    "   - 訓練 3 個 epochs。\n",
    "4. **實驗結果**：\n",
    "   - **Val set Macro F1 Score**: `0.4815`\n",
    "   - **Public leaderboard**: `0.42491`\n",
    "   - **Private leaderboard**: `0.41350`\n",
    "5. **評價**：\n",
    "   - 得到了一個中偏下的排名與成績，可能是因為 undersampling 使數據量過小，模型無法有效學習。\n",
    "\n",
    "---\n",
    "\n",
    "#### **實驗 2**\n",
    "\n",
    "1. 不進行 sampling，使用完整數據進行訓練。\n",
    "2. 針對數據分佈不均的情況，將損失函數從 **CrossEntropyLoss** 改為 **Focal Loss**，以期更好應對類別不平衡的情況。\n",
    "3. 訓練參數：\n",
    "   - 訓練 3 個 epochs。\n",
    "4. **實驗結果**：\n",
    "   - **Val set Macro F1 Score**: `0.5431`（優於實驗 1）。\n",
    "   - **Public leaderboard**: `0.48862`\n",
    "   - **Private leaderboard**: `0.47377`\n",
    "5. **評價**：\n",
    "   - 得到了一個中間排名與成績，數據量的增加讓模型學習到更多的特徵，提升了判斷能力。\n",
    "\n",
    "---\n",
    "\n",
    "### **可能的未來改進方向**\n",
    "\n",
    "- **模型改進**：\n",
    "  - 考慮使用參數量更大的模型，如 `roberta-large` 或其他更強的 Transformer 架構。\n",
    "  \n",
    "- **數據增強**：\n",
    "  - 嘗試對數據進行數據增強（Data Augmentation），提升模型對低頻類別的學習能力。\n",
    "\n",
    "- **Warm-up 策略**：\n",
    "  - 先用部分數據進行 **Warm-up**，幫助模型快速適應任務，建立更穩定的基礎。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _score          _index                                            _source  \\\n",
      "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
      "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
      "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
      "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
      "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
      "\n",
      "            _crawldate   _type  \n",
      "0  2015-05-23 11:42:47  tweets  \n",
      "1  2016-01-28 04:52:09  tweets  \n",
      "2  2017-12-25 04:39:20  tweets  \n",
      "3  2016-01-24 23:53:05  tweets  \n",
      "4  2016-01-08 17:18:59  tweets  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 JSON Lines 格式文件\n",
    "tweets_df = pd.read_json('tweets_DM.json', lines=True)\n",
    "\n",
    "# 查看前幾行\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id                                               text\n",
      "0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
      "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
      "2  0x28b412  Confident of your obedience, I write to you, k...\n",
      "3  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>\n",
      "4  0x2de201  \"Trust is not the same as faith. A friend is s...\n"
     ]
    }
   ],
   "source": [
    "# 讀取 JSON，僅提取 tweet_id 和 text\n",
    "tweets = []\n",
    "with open('tweets_DM.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        tweet_data = json.loads(line)\n",
    "        tweet_id = tweet_data['_source']['tweet']['tweet_id']\n",
    "        text = tweet_data['_source']['tweet']['text']\n",
    "        tweets.append({'tweet_id': tweet_id, 'text': text})\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id identification                                               text  \\\n",
      "0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n",
      "1  0x29e452          train  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n",
      "2  0x2b3819          train  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  0x2a2acc          train  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "  emotion  \n",
      "0     NaN  \n",
      "1     joy  \n",
      "2     joy  \n",
      "3     NaN  \n",
      "4   trust  \n"
     ]
    }
   ],
   "source": [
    "# emotion.csv: 標籤資料\n",
    "emotion = pd.read_csv('emotion.csv')\n",
    "\n",
    "# data_identification.csv: 區分 train/test\n",
    "data_identification = pd.read_csv('data_identification.csv')\n",
    "\n",
    "df = data_identification.merge(tweets_df, on='tweet_id', how='left')\n",
    "df = df.merge(emotion, on='tweet_id', how='left')  \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各情感類別的數量：\n",
      "emotion\n",
      "joy             516017\n",
      "anticipation    248935\n",
      "trust           205478\n",
      "sadness         193437\n",
      "disgust         139101\n",
      "fear             63999\n",
      "surprise         48729\n",
      "anger            39867\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 確認 `emotion` 欄位中各類別的數量\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "\n",
    "print(\"各情感類別的數量：\")\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 可以看到，各情緒類別的占比其實差異滿大的，joy 占了很大部分\n",
    "\n",
    "--> 有確認 train set 中沒有情緒 attribute 是空值的狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各情感類別的數量：\n",
      "identification\n",
      "train    1455563\n",
      "test      411972\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type_counts = df['identification'].value_counts()\n",
    "\n",
    "print(\"各情感類別的數量：\")\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 可以看到，train 的數據量很大，應該可以給模型很多樣的資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2452cf</td>\n",
       "      <td>train</td>\n",
       "      <td>Never give up. The manifestation of your goal ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x2d729d</td>\n",
       "      <td>train</td>\n",
       "      <td>I Believe When No One Else Does... &lt;LH&gt; #Dream...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x2ab56d</td>\n",
       "      <td>train</td>\n",
       "      <td>@SirPareshRawal with due respect... Do u have ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0x1f3657</td>\n",
       "      <td>train</td>\n",
       "      <td>I can't tell if I'm alive or in the after life...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0x1fcc53</td>\n",
       "      <td>train</td>\n",
       "      <td>#GRATEFUL!!  WORLD GOODMORNING!</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0x254da9</td>\n",
       "      <td>train</td>\n",
       "      <td>@happinessplannr #LoveIsLove &lt;LH&gt; this!! 👩‍❤️‍👩👭</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0x2e19fe</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Santa Claus with my parents❤ &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0x37c6da</td>\n",
       "      <td>train</td>\n",
       "      <td>Encounters with the &lt;LH&gt; of God will change yo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0x2c3d04</td>\n",
       "      <td>train</td>\n",
       "      <td>95° or higher 17 out of the last 19 days, no r...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0x1e414d</td>\n",
       "      <td>train</td>\n",
       "      <td>#Congratulations to all our #Graduates from @S...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0x1f0ab5</td>\n",
       "      <td>train</td>\n",
       "      <td>Hustle Season Has Return &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0x275fcc</td>\n",
       "      <td>train</td>\n",
       "      <td>3nights out this weekend has ruined me. Twiste...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id identification  \\\n",
       "0   0x28cc61           test   \n",
       "1   0x29e452          train   \n",
       "2   0x2b3819          train   \n",
       "3   0x2db41f           test   \n",
       "4   0x2a2acc          train   \n",
       "5   0x2a8830          train   \n",
       "6   0x20b21d          train   \n",
       "7   0x2452cf          train   \n",
       "8   0x2d729d          train   \n",
       "9   0x2ab56d          train   \n",
       "10  0x1f3657          train   \n",
       "11  0x1fcc53          train   \n",
       "12  0x254da9          train   \n",
       "13  0x2e19fe          train   \n",
       "14  0x37c6da          train   \n",
       "15  0x2466f6           test   \n",
       "16  0x2c3d04          train   \n",
       "17  0x1e414d          train   \n",
       "18  0x1f0ab5          train   \n",
       "19  0x275fcc          train   \n",
       "\n",
       "                                                 text       emotion  \n",
       "0   @Habbo I've seen two separate colours of the e...           NaN  \n",
       "1   Huge Respect🖒 @JohnnyVegasReal talking about l...           joy  \n",
       "2   Yoooo we hit all our monthly goals with the ne...           joy  \n",
       "3   @FoxNews @KellyannePolls No serious self respe...           NaN  \n",
       "4   @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...         trust  \n",
       "5   Come join @ambushman27 on #PUBG while he striv...           joy  \n",
       "6   @fanshixieen2014 Blessings!My #strength little...  anticipation  \n",
       "7   Never give up. The manifestation of your goal ...  anticipation  \n",
       "8   I Believe When No One Else Does... <LH> #Dream...  anticipation  \n",
       "9   @SirPareshRawal with due respect... Do u have ...           joy  \n",
       "10  I can't tell if I'm alive or in the after life...       sadness  \n",
       "11                    #GRATEFUL!!  WORLD GOODMORNING!         trust  \n",
       "12   @happinessplannr #LoveIsLove <LH> this!! 👩‍❤️‍👩👭           joy  \n",
       "13         Watching Santa Claus with my parents❤ <LH>           joy  \n",
       "14  Encounters with the <LH> of God will change yo...           joy  \n",
       "15  Looking for a new car, and it says 1 lady owne...           NaN  \n",
       "16  95° or higher 17 out of the last 19 days, no r...       disgust  \n",
       "17  #Congratulations to all our #Graduates from @S...         trust  \n",
       "18                      Hustle Season Has Return <LH>  anticipation  \n",
       "19  3nights out this weekend has ruined me. Twiste...       sadness  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料清理\n",
    "\n",
    "定義清理函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考量到推特文章會有一些特性，像是包含URL、tag 其他使用者等等，故作了一個相對應的前處理功能，以提供一個比較不會有太誤導狀況的資料給BERT做後續分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始推文： @user I absolutely LOVE the new #iPhone12!!! It's soooo amazing! Check it out at https://apple.com\n",
      "BERT 前處理後的推文：  i absolutely love the new iphone12!!! it's soooo amazing! check it out at \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 下載必要的 NLTK 資源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化停用詞集合和詞形還原器\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_for_bert(tweet):\n",
    "    # 1. 移除 URL\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    # 2. 移除用戶提及\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # 3. 移除 Emoji\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "    # 4. 保留標籤內容，移除 `#`\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # 5. 統一轉換為小寫（適用於 uncased 模型）\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "# 範例推文\n",
    "tweet = \"@user I absolutely LOVE the new #iPhone12!!! It's soooo amazing! Check it out at https://apple.com\"\n",
    "processed_tweet = preprocess_for_bert(tweet)\n",
    "print(\"原始推文：\", tweet)\n",
    "print(\"BERT 前處理後的推文：\", processed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @Habbo I've seen two separate colours of the e...   \n",
      "1  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n",
      "2  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0   i've seen two separate colours of the elegant...  \n",
      "1  huge respect  talking about losing his dad to ...  \n",
      "2  yoooo we hit all our monthly goals with the ne...  \n",
      "3    no serious self respecting individual believ...  \n",
      "4          well done team  <lh> of every one of you.  \n"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df['text'].apply(preprocess_for_bert)\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i've seen two separate colours of the elegant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "      <td>huge respect  talking about losing his dad to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "      <td>yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no serious self respecting individual believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "      <td>well done team  &lt;lh&gt; of every one of you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification                                               text  \\\n",
       "0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n",
       "1  0x29e452          train  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n",
       "2  0x2b3819          train  Yoooo we hit all our monthly goals with the ne...   \n",
       "3  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n",
       "4  0x2a2acc          train  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
       "\n",
       "  emotion                                       cleaned_text  \n",
       "0     NaN   i've seen two separate colours of the elegant...  \n",
       "1     joy  huge respect  talking about losing his dad to ...  \n",
       "2     joy  yoooo we hit all our monthly goals with the ne...  \n",
       "3     NaN    no serious self respecting individual believ...  \n",
       "4   trust          well done team  <lh> of every one of you.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前處理完成，將 train & test set 分開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame 大小: (1455563, 5)\n",
      "Test DataFrame 大小: (411972, 5)\n"
     ]
    }
   ],
   "source": [
    "# 分割成 train 和 test 的 DataFrame\n",
    "train_df_ttl = df[df['identification'] == 'train']\n",
    "test_df = df[df['identification'] == 'test']\n",
    "\n",
    "# 檢查分割後的結果\n",
    "print(f\"Train DataFrame 大小: {train_df_ttl.shape}\")\n",
    "print(f\"Test DataFrame 大小: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 BERT 抓取特徵並進行分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Using GPU\" if torch.cuda.is_available() else \"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型選用: distilbert-base-uncased\n",
    "\n",
    "選用原因: 因為手邊的硬體資源較不足，經查詢\"distilbert-base-uncased\" 是一基於 BERT (bert-base-uncased) 的蒸餾版本，其參數量較小，且訓練跟推論數度都更快一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實驗1: \n",
    "考量到各類別訓練資料不平均的狀況，採用 undersampling\n",
    "\n",
    "同時也藉由降低數據量，希望模型不要 train 太久， 能盡快看到一個成績當作 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# 1. 確認資料格式\n",
    "print(\"準備資料中...\")\n",
    "print(train_df_ttl.head())\n",
    "\n",
    "# 2. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_ttl['label'] = label_encoder.fit_transform(train_df_ttl['emotion'])  # 把情感文字轉換為數值\n",
    "\n",
    "# 3. Train-Test Split\n",
    "train_df, val_df = train_test_split(train_df_ttl, test_size=0.2, stratify=train_df_ttl['label'], random_state=42)\n",
    "print(f\"訓練集大小: {len(train_df)}, 驗證集大小: {len(val_df)}\")\n",
    "\n",
    "\n",
    "# 4. Under Sampling 函數\n",
    "def undersample_data(df, label_col):\n",
    "    min_count = df[label_col].value_counts().min()  # 最小樣本數\n",
    "    balanced_dfs = []\n",
    "    for label in df[label_col].unique():\n",
    "        class_subset = df[df[label_col] == label]\n",
    "        balanced_dfs.append(resample(class_subset, replace=False, n_samples=min_count, random_state=42))\n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    return balanced_df\n",
    "\n",
    "# 進行 Under Sampling\n",
    "train_df_balanced = undersample_data(train_df, 'label')\n",
    "print(f\"平衡後的訓練集大小: {len(train_df_balanced)}\")\n",
    "\n",
    "# 5. 使用 DistilBERT 的 tokenizer \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 6. 定義數據集處理類別\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        label = row['label']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 建立數據集與 DataLoader\n",
    "train_dataset = EmotionDataset(train_df_balanced, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 7. 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# 8. 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 9. 訓練函數\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 10. 驗證函數（計算 Macro F1 Score）\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(dataloader), f1\n",
    "\n",
    "# 11. 主訓練迴圈\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # 執行訓練\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    # 執行驗證\n",
    "    val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    # 輸出訓練與驗證結果\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Macro F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    epoch_dir = f'./distilbert_emotion_model_{epoch + 1}'\n",
    "    model.save_pretrained(epoch_dir)\n",
    "    tokenizer.save_pretrained(epoch_dir)\n",
    "    print(f\"模型已保存至 {epoch_dir}！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic2](img/pic2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "總共訓練了3個 epochs,因為第3個 epoch 的表現最好，故繼續往下進行test set 預測，沒有額外再切換模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 進行 Test Set 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定義測試數據集處理類別\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return item\n",
    "\n",
    "# 2. 構建測試 DataLoader\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 3. 預測函數\n",
    "def predict_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Predicting\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# 4. 對測試數據進行預測\n",
    "print(\"開始對測試數據進行預測...\")\n",
    "test_predictions = predict_model(model, test_loader, device)\n",
    "\n",
    "# 5. 將預測結果轉換為原始情感標籤\n",
    "test_df['predicted_emotion'] = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# 6. 儲存預測結果\n",
    "'''\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']]  # 只保留 tweet_id 和 emotion 欄位\n",
    "submission.to_csv('submission.csv', index=False)  # 儲存為 submission.csv\n",
    "print(\"測試數據的預測結果已保存為 'submission.csv'\")\n",
    "'''\n",
    "# 重命名欄位\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']].rename(columns={\n",
    "    'tweet_id': 'id',\n",
    "    'predicted_emotion': 'emotion'\n",
    "})\n",
    "\n",
    "# 儲存為 submission.csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"測試數據的預測結果已保存為 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此預測結果獲得 public score: 0.42491\n",
    "\n",
    "算是一個中偏下的排名與成績\n",
    "\n",
    "可能是因為undersampling 讓數據量變得太小，可能模型不太好學習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic1](img/pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實驗2: \n",
    "\n",
    "不做sampling, 使用全部的數據去訓練\n",
    "\n",
    "但依舊考量到數據量不平均的狀況，故將損失評估從 CrossEntropyLoss 改成 focal loss，以期更好應對類別不平衡情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. 確認資料格式\n",
    "print(\"準備資料中...\")\n",
    "print(train_df_ttl.head())\n",
    "\n",
    "# Label Encoding 情感標籤\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_ttl['label'] = label_encoder.fit_transform(train_df_ttl['emotion'])  # 把情感文字轉換為數值\n",
    "\n",
    "# 3. Train-Test Split\n",
    "train_df, val_df = train_test_split(train_df_ttl, test_size=0.2, stratify=train_df_ttl['label'], random_state=42)\n",
    "print(f\"訓練集大小: {len(train_df)}, 驗證集大小: {len(val_df)}\")\n",
    "\n",
    "# 4. 使用 DistilBERT 的 tokenizer \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 5. 定義數據集處理類別\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        label = row['label']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "\n",
    "# 6. 構建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 7. 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# 8. 定義 Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities for true labels\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss()  # 使用 Focal Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 9. 訓練函數\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 10. 驗證函數（使用 F1 Score）\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    return f1\n",
    "\n",
    "# 11. 主訓練迴圈\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # 執行訓練\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    # 執行驗證\n",
    "    val_f1 = eval_model(model, val_loader, device)\n",
    "    \n",
    "    # 輸出訓練與驗證結果\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    epoch_dir = f'./distilbert_emotion_model_{epoch + 1}'\n",
    "    model.save_pretrained(epoch_dir)\n",
    "    tokenizer.save_pretrained(epoch_dir)\n",
    "    print(f\"模型已保存至 {epoch_dir}！\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic3](img/pic3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在訓練中的 F1驗證成績已經看到比實驗1還要好\n",
    "\n",
    "一樣，總共訓練了3個 epochs,因為第3個 epoch 的表現最好，故繼續往下進行test set 預測，沒有額外再切換模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做Test Set 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定義測試數據集處理類別\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return item\n",
    "\n",
    "# 2. 構建測試 DataLoader\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 3. 預測函數\n",
    "def predict_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Predicting\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# 4. 對測試數據進行預測\n",
    "print(\"開始對測試數據進行預測...\")\n",
    "test_predictions = predict_model(model, test_loader, device)\n",
    "\n",
    "# 5. 將預測結果轉換為原始情感標籤\n",
    "test_df['predicted_emotion'] = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# 6. 儲存預測結果\n",
    "'''\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']]  # 只保留 tweet_id 和 emotion 欄位\n",
    "submission.to_csv('submission.csv', index=False)  # 儲存為 submission.csv\n",
    "print(\"測試數據的預測結果已保存為 'submission.csv'\")\n",
    "'''\n",
    "# 重命名欄位\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']].rename(columns={\n",
    "    'tweet_id': 'id',\n",
    "    'predicted_emotion': 'emotion'\n",
    "})\n",
    "\n",
    "# 儲存為 submission.csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"測試數據的預測結果已保存為 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此預測結果獲得 public score: 0.48862\n",
    "\n",
    "算是一個中間的排名與成績"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic4](img/pic4.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
