{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: å³å›æ…§\n",
    "\n",
    "Student ID: 113065539\n",
    "\n",
    "GitHub ID: Iris6636\n",
    "\n",
    "Kaggle name: Rondnoir\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "![pic0](img/pic0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY**\n",
    "\n",
    "---\n",
    "\n",
    "### **é è™•ç†**\n",
    "\n",
    "è€ƒé‡åˆ°æ¨ç‰¹æ–‡ç« çš„ç‰¹æ€§ï¼Œä¾‹å¦‚åŒ…å« URLã€@tag ç”¨æˆ¶ç­‰ï¼Œç‚ºé¿å…é€™äº›å…ƒç´ å°æƒ…ç·’åˆ†é¡çš„å½±éŸ¿ï¼Œè¨­è¨ˆäº†ç›¸æ‡‰çš„å‰è™•ç†æµç¨‹ï¼Œç›®çš„æ˜¯æä¾›æ›´ä¹¾æ·¨ä¸”ä¸æ˜“èª¤å°çš„æ•¸æ“šçµ¦ BERT æ¨¡å‹é€²è¡Œå¾ŒçºŒåˆ†æã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **è³‡æ–™åŸºæœ¬åˆ†æ**\n",
    "\n",
    "1. **æƒ…ç·’é¡åˆ¥å æ¯”**ï¼š\n",
    "   - å„æƒ…ç·’é¡åˆ¥åˆ†ä½ˆå·®ç•°è¼ƒå¤§ï¼Œå…¶ä¸­ `joy` é¡åˆ¥å æ¯”æœ€é«˜ï¼Œæ˜é¡¯å¤šæ–¼å…¶ä»–é¡åˆ¥ã€‚\n",
    "   \n",
    "2. **æ•¸æ“šå®Œæ•´æ€§**ï¼š\n",
    "   - ç¶“éæª¢æŸ¥ï¼Œ`train set` ä¸­æ²’æœ‰æƒ…ç·’æ¨™ç±¤ (`emotion attribute`) çš„ç©ºå€¼æƒ…æ³ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **æ¨¡å‹é¸ç”¨**\n",
    "\n",
    "ç‚ºäº†æ›´å¥½åœ°æ•æ‰æ¨æ–‡ä¸­çš„æƒ…ç·’ç‰¹å¾µï¼Œé¸ç”¨äº†åŸºæ–¼ Transformer æ¶æ§‹çš„æ¨¡å‹ï¼š\n",
    "- **æ¨¡å‹é¸æ“‡åŸå› **ï¼š\n",
    "  - ä½¿ç”¨ `distilbert-base-uncased` æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹åŸºæ–¼ BERT (`bert-base-uncased`) çš„è’¸é¤¾ç‰ˆæœ¬ï¼Œå…·å‚™ä»¥ä¸‹å„ªå‹¢ï¼š\n",
    "    1. åƒæ•¸é‡åƒ…ç‚ºåŸæ¨¡å‹çš„ 60%ï¼Œè¼•é‡åŒ–ä¸”é«˜æ•ˆã€‚\n",
    "    2. è¨“ç·´é€Ÿåº¦èˆ‡æ¨è«–é€Ÿåº¦æå‡ç´„ 40%ï¼Œéå¸¸é©åˆç¡¬é«”è³‡æºæœ‰é™çš„ç’°å¢ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å¯¦é©—**\n",
    "\n",
    "#### **å¯¦é©— 1**\n",
    "\n",
    "1. è€ƒé‡åˆ°å„é¡åˆ¥è¨“ç·´è³‡æ–™ä¸å¹³å‡çš„ç‹€æ³ï¼Œæ¡ç”¨ **undersampling**ã€‚\n",
    "2. è—‰ç”±é™ä½æ•¸æ“šé‡ï¼Œå¸Œæœ›æ¨¡å‹è¨“ç·´æ™‚é–“ç¸®çŸ­ï¼Œèƒ½ç›¡å¿«å¾—åˆ°ä¸€å€‹æˆç¸¾ä½œç‚º baselineã€‚\n",
    "3. è¨“ç·´åƒæ•¸ï¼š\n",
    "   - è¨“ç·´ 3 å€‹ epochsã€‚\n",
    "4. **å¯¦é©—çµæœ**ï¼š\n",
    "   - **Val set Macro F1 Score**: `0.4815`\n",
    "   - **Public leaderboard**: `0.42491`\n",
    "   - **Private leaderboard**: `0.41350`\n",
    "5. **è©•åƒ¹**ï¼š\n",
    "   - å¾—åˆ°äº†ä¸€å€‹ä¸­åä¸‹çš„æ’åèˆ‡æˆç¸¾ï¼Œå¯èƒ½æ˜¯å› ç‚º undersampling ä½¿æ•¸æ“šé‡éå°ï¼Œæ¨¡å‹ç„¡æ³•æœ‰æ•ˆå­¸ç¿’ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **å¯¦é©— 2**\n",
    "\n",
    "1. ä¸é€²è¡Œ samplingï¼Œä½¿ç”¨å®Œæ•´æ•¸æ“šé€²è¡Œè¨“ç·´ã€‚\n",
    "2. é‡å°æ•¸æ“šåˆ†ä½ˆä¸å‡çš„æƒ…æ³ï¼Œå°‡æå¤±å‡½æ•¸å¾ **CrossEntropyLoss** æ”¹ç‚º **Focal Loss**ï¼Œä»¥æœŸæ›´å¥½æ‡‰å°é¡åˆ¥ä¸å¹³è¡¡çš„æƒ…æ³ã€‚\n",
    "3. è¨“ç·´åƒæ•¸ï¼š\n",
    "   - è¨“ç·´ 3 å€‹ epochsã€‚\n",
    "4. **å¯¦é©—çµæœ**ï¼š\n",
    "   - **Val set Macro F1 Score**: `0.5431`ï¼ˆå„ªæ–¼å¯¦é©— 1ï¼‰ã€‚\n",
    "   - **Public leaderboard**: `0.48862`\n",
    "   - **Private leaderboard**: `0.47377`\n",
    "5. **è©•åƒ¹**ï¼š\n",
    "   - å¾—åˆ°äº†ä¸€å€‹ä¸­é–“æ’åèˆ‡æˆç¸¾ï¼Œæ•¸æ“šé‡çš„å¢åŠ è®“æ¨¡å‹å­¸ç¿’åˆ°æ›´å¤šçš„ç‰¹å¾µï¼Œæå‡äº†åˆ¤æ–·èƒ½åŠ›ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å¯èƒ½çš„æœªä¾†æ”¹é€²æ–¹å‘**\n",
    "\n",
    "- **æ¨¡å‹æ”¹é€²**ï¼š\n",
    "  - è€ƒæ…®ä½¿ç”¨åƒæ•¸é‡æ›´å¤§çš„æ¨¡å‹ï¼Œå¦‚ `roberta-large` æˆ–å…¶ä»–æ›´å¼·çš„ Transformer æ¶æ§‹ã€‚\n",
    "  \n",
    "- **æ•¸æ“šå¢å¼·**ï¼š\n",
    "  - å˜—è©¦å°æ•¸æ“šé€²è¡Œæ•¸æ“šå¢å¼·ï¼ˆData Augmentationï¼‰ï¼Œæå‡æ¨¡å‹å°ä½é »é¡åˆ¥çš„å­¸ç¿’èƒ½åŠ›ã€‚\n",
    "\n",
    "- **Warm-up ç­–ç•¥**ï¼š\n",
    "  - å…ˆç”¨éƒ¨åˆ†æ•¸æ“šé€²è¡Œ **Warm-up**ï¼Œå¹«åŠ©æ¨¡å‹å¿«é€Ÿé©æ‡‰ä»»å‹™ï¼Œå»ºç«‹æ›´ç©©å®šçš„åŸºç¤ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è®€å–è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _score          _index                                            _source  \\\n",
      "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
      "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
      "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
      "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
      "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
      "\n",
      "            _crawldate   _type  \n",
      "0  2015-05-23 11:42:47  tweets  \n",
      "1  2016-01-28 04:52:09  tweets  \n",
      "2  2017-12-25 04:39:20  tweets  \n",
      "3  2016-01-24 23:53:05  tweets  \n",
      "4  2016-01-08 17:18:59  tweets  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è®€å– JSON Lines æ ¼å¼æ–‡ä»¶\n",
    "tweets_df = pd.read_json('tweets_DM.json', lines=True)\n",
    "\n",
    "# æŸ¥çœ‹å‰å¹¾è¡Œ\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id                                               text\n",
      "0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
      "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
      "2  0x28b412  Confident of your obedience, I write to you, k...\n",
      "3  0x1cd5b0                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>\n",
      "4  0x2de201  \"Trust is not the same as faith. A friend is s...\n"
     ]
    }
   ],
   "source": [
    "# è®€å– JSONï¼Œåƒ…æå– tweet_id å’Œ text\n",
    "tweets = []\n",
    "with open('tweets_DM.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        tweet_data = json.loads(line)\n",
    "        tweet_id = tweet_data['_source']['tweet']['tweet_id']\n",
    "        text = tweet_data['_source']['tweet']['text']\n",
    "        tweets.append({'tweet_id': tweet_id, 'text': text})\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id identification                                               text  \\\n",
      "0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n",
      "1  0x29e452          train  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n",
      "2  0x2b3819          train  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  0x2a2acc          train  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "  emotion  \n",
      "0     NaN  \n",
      "1     joy  \n",
      "2     joy  \n",
      "3     NaN  \n",
      "4   trust  \n"
     ]
    }
   ],
   "source": [
    "# emotion.csv: æ¨™ç±¤è³‡æ–™\n",
    "emotion = pd.read_csv('emotion.csv')\n",
    "\n",
    "# data_identification.csv: å€åˆ† train/test\n",
    "data_identification = pd.read_csv('data_identification.csv')\n",
    "\n",
    "df = data_identification.merge(tweets_df, on='tweet_id', how='left')\n",
    "df = df.merge(emotion, on='tweet_id', how='left')  \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„æƒ…æ„Ÿé¡åˆ¥çš„æ•¸é‡ï¼š\n",
      "emotion\n",
      "joy             516017\n",
      "anticipation    248935\n",
      "trust           205478\n",
      "sadness         193437\n",
      "disgust         139101\n",
      "fear             63999\n",
      "surprise         48729\n",
      "anger            39867\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ç¢ºèª `emotion` æ¬„ä½ä¸­å„é¡åˆ¥çš„æ•¸é‡\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "\n",
    "print(\"å„æƒ…æ„Ÿé¡åˆ¥çš„æ•¸é‡ï¼š\")\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> å¯ä»¥çœ‹åˆ°ï¼Œå„æƒ…ç·’é¡åˆ¥çš„å æ¯”å…¶å¯¦å·®ç•°æ»¿å¤§çš„ï¼Œjoy å äº†å¾ˆå¤§éƒ¨åˆ†\n",
    "\n",
    "--> æœ‰ç¢ºèª train set ä¸­æ²’æœ‰æƒ…ç·’ attribute æ˜¯ç©ºå€¼çš„ç‹€æ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„æƒ…æ„Ÿé¡åˆ¥çš„æ•¸é‡ï¼š\n",
      "identification\n",
      "train    1455563\n",
      "test      411972\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type_counts = df['identification'].value_counts()\n",
    "\n",
    "print(\"å„æƒ…æ„Ÿé¡åˆ¥çš„æ•¸é‡ï¼š\")\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> å¯ä»¥çœ‹åˆ°ï¼Œtrain çš„æ•¸æ“šé‡å¾ˆå¤§ï¼Œæ‡‰è©²å¯ä»¥çµ¦æ¨¡å‹å¾ˆå¤šæ¨£çš„è³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge RespectğŸ–’ @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2452cf</td>\n",
       "      <td>train</td>\n",
       "      <td>Never give up. The manifestation of your goal ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x2d729d</td>\n",
       "      <td>train</td>\n",
       "      <td>I Believe When No One Else Does... &lt;LH&gt; #Dream...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x2ab56d</td>\n",
       "      <td>train</td>\n",
       "      <td>@SirPareshRawal with due respect... Do u have ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0x1f3657</td>\n",
       "      <td>train</td>\n",
       "      <td>I can't tell if I'm alive or in the after life...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0x1fcc53</td>\n",
       "      <td>train</td>\n",
       "      <td>#GRATEFUL!!  WORLD GOODMORNING!</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0x254da9</td>\n",
       "      <td>train</td>\n",
       "      <td>@happinessplannr #LoveIsLove &lt;LH&gt; this!! ğŸ‘©â€â¤ï¸â€ğŸ‘©ğŸ‘­</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0x2e19fe</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Santa Claus with my parentsâ¤ &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0x37c6da</td>\n",
       "      <td>train</td>\n",
       "      <td>Encounters with the &lt;LH&gt; of God will change yo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0x2c3d04</td>\n",
       "      <td>train</td>\n",
       "      <td>95Â° or higher 17 out of the last 19 days, no r...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0x1e414d</td>\n",
       "      <td>train</td>\n",
       "      <td>#Congratulations to all our #Graduates from @S...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0x1f0ab5</td>\n",
       "      <td>train</td>\n",
       "      <td>Hustle Season Has Return &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0x275fcc</td>\n",
       "      <td>train</td>\n",
       "      <td>3nights out this weekend has ruined me. Twiste...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id identification  \\\n",
       "0   0x28cc61           test   \n",
       "1   0x29e452          train   \n",
       "2   0x2b3819          train   \n",
       "3   0x2db41f           test   \n",
       "4   0x2a2acc          train   \n",
       "5   0x2a8830          train   \n",
       "6   0x20b21d          train   \n",
       "7   0x2452cf          train   \n",
       "8   0x2d729d          train   \n",
       "9   0x2ab56d          train   \n",
       "10  0x1f3657          train   \n",
       "11  0x1fcc53          train   \n",
       "12  0x254da9          train   \n",
       "13  0x2e19fe          train   \n",
       "14  0x37c6da          train   \n",
       "15  0x2466f6           test   \n",
       "16  0x2c3d04          train   \n",
       "17  0x1e414d          train   \n",
       "18  0x1f0ab5          train   \n",
       "19  0x275fcc          train   \n",
       "\n",
       "                                                 text       emotion  \n",
       "0   @Habbo I've seen two separate colours of the e...           NaN  \n",
       "1   Huge RespectğŸ–’ @JohnnyVegasReal talking about l...           joy  \n",
       "2   Yoooo we hit all our monthly goals with the ne...           joy  \n",
       "3   @FoxNews @KellyannePolls No serious self respe...           NaN  \n",
       "4   @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...         trust  \n",
       "5   Come join @ambushman27 on #PUBG while he striv...           joy  \n",
       "6   @fanshixieen2014 Blessings!My #strength little...  anticipation  \n",
       "7   Never give up. The manifestation of your goal ...  anticipation  \n",
       "8   I Believe When No One Else Does... <LH> #Dream...  anticipation  \n",
       "9   @SirPareshRawal with due respect... Do u have ...           joy  \n",
       "10  I can't tell if I'm alive or in the after life...       sadness  \n",
       "11                    #GRATEFUL!!  WORLD GOODMORNING!         trust  \n",
       "12   @happinessplannr #LoveIsLove <LH> this!! ğŸ‘©â€â¤ï¸â€ğŸ‘©ğŸ‘­           joy  \n",
       "13         Watching Santa Claus with my parentsâ¤ <LH>           joy  \n",
       "14  Encounters with the <LH> of God will change yo...           joy  \n",
       "15  Looking for a new car, and it says 1 lady owne...           NaN  \n",
       "16  95Â° or higher 17 out of the last 19 days, no r...       disgust  \n",
       "17  #Congratulations to all our #Graduates from @S...         trust  \n",
       "18                      Hustle Season Has Return <LH>  anticipation  \n",
       "19  3nights out this weekend has ruined me. Twiste...       sadness  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è³‡æ–™æ¸…ç†\n",
    "\n",
    "å®šç¾©æ¸…ç†å‡½æ•¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è€ƒé‡åˆ°æ¨ç‰¹æ–‡ç« æœƒæœ‰ä¸€äº›ç‰¹æ€§ï¼Œåƒæ˜¯åŒ…å«URLã€tag å…¶ä»–ä½¿ç”¨è€…ç­‰ç­‰ï¼Œæ•…ä½œäº†ä¸€å€‹ç›¸å°æ‡‰çš„å‰è™•ç†åŠŸèƒ½ï¼Œä»¥æä¾›ä¸€å€‹æ¯”è¼ƒä¸æœƒæœ‰å¤ªèª¤å°ç‹€æ³çš„è³‡æ–™çµ¦BERTåšå¾ŒçºŒåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ¨æ–‡ï¼š @user I absolutely LOVE the new #iPhone12!!! It's soooo amazing! Check it out at https://apple.com\n",
      "BERT å‰è™•ç†å¾Œçš„æ¨æ–‡ï¼š  i absolutely love the new iphone12!!! it's soooo amazing! check it out at \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thpss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ä¸‹è¼‰å¿…è¦çš„ NLTK è³‡æº\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# åˆå§‹åŒ–åœç”¨è©é›†åˆå’Œè©å½¢é‚„åŸå™¨\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_for_bert(tweet):\n",
    "    # 1. ç§»é™¤ URL\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    # 2. ç§»é™¤ç”¨æˆ¶æåŠ\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # 3. ç§»é™¤ Emoji\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "    # 4. ä¿ç•™æ¨™ç±¤å…§å®¹ï¼Œç§»é™¤ `#`\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # 5. çµ±ä¸€è½‰æ›ç‚ºå°å¯«ï¼ˆé©ç”¨æ–¼ uncased æ¨¡å‹ï¼‰\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "# ç¯„ä¾‹æ¨æ–‡\n",
    "tweet = \"@user I absolutely LOVE the new #iPhone12!!! It's soooo amazing! Check it out at https://apple.com\"\n",
    "processed_tweet = preprocess_for_bert(tweet)\n",
    "print(\"åŸå§‹æ¨æ–‡ï¼š\", tweet)\n",
    "print(\"BERT å‰è™•ç†å¾Œçš„æ¨æ–‡ï¼š\", processed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @Habbo I've seen two separate colours of the e...   \n",
      "1  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n",
      "2  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0   i've seen two separate colours of the elegant...  \n",
      "1  huge respect  talking about losing his dad to ...  \n",
      "2  yoooo we hit all our monthly goals with the ne...  \n",
      "3    no serious self respecting individual believ...  \n",
      "4          well done team  <lh> of every one of you.  \n"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df['text'].apply(preprocess_for_bert)\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i've seen two separate colours of the elegant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge RespectğŸ–’ @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "      <td>huge respect  talking about losing his dad to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "      <td>yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no serious self respecting individual believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "      <td>well done team  &lt;lh&gt; of every one of you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification                                               text  \\\n",
       "0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n",
       "1  0x29e452          train  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n",
       "2  0x2b3819          train  Yoooo we hit all our monthly goals with the ne...   \n",
       "3  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n",
       "4  0x2a2acc          train  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
       "\n",
       "  emotion                                       cleaned_text  \n",
       "0     NaN   i've seen two separate colours of the elegant...  \n",
       "1     joy  huge respect  talking about losing his dad to ...  \n",
       "2     joy  yoooo we hit all our monthly goals with the ne...  \n",
       "3     NaN    no serious self respecting individual believ...  \n",
       "4   trust          well done team  <lh> of every one of you.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰è™•ç†å®Œæˆï¼Œå°‡ train & test set åˆ†é–‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame å¤§å°: (1455563, 5)\n",
      "Test DataFrame å¤§å°: (411972, 5)\n"
     ]
    }
   ],
   "source": [
    "# åˆ†å‰²æˆ train å’Œ test çš„ DataFrame\n",
    "train_df_ttl = df[df['identification'] == 'train']\n",
    "test_df = df[df['identification'] == 'test']\n",
    "\n",
    "# æª¢æŸ¥åˆ†å‰²å¾Œçš„çµæœ\n",
    "print(f\"Train DataFrame å¤§å°: {train_df_ttl.shape}\")\n",
    "print(f\"Test DataFrame å¤§å°: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨ BERT æŠ“å–ç‰¹å¾µä¸¦é€²è¡Œåˆ†é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Using GPU\" if torch.cuda.is_available() else \"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹é¸ç”¨: distilbert-base-uncased\n",
    "\n",
    "é¸ç”¨åŸå› : å› ç‚ºæ‰‹é‚Šçš„ç¡¬é«”è³‡æºè¼ƒä¸è¶³ï¼Œç¶“æŸ¥è©¢\"distilbert-base-uncased\" æ˜¯ä¸€åŸºæ–¼ BERT (bert-base-uncased) çš„è’¸é¤¾ç‰ˆæœ¬ï¼Œå…¶åƒæ•¸é‡è¼ƒå°ï¼Œä¸”è¨“ç·´è·Ÿæ¨è«–æ•¸åº¦éƒ½æ›´å¿«ä¸€äº›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¦é©—1: \n",
    "è€ƒé‡åˆ°å„é¡åˆ¥è¨“ç·´è³‡æ–™ä¸å¹³å‡çš„ç‹€æ³ï¼Œæ¡ç”¨ undersampling\n",
    "\n",
    "åŒæ™‚ä¹Ÿè—‰ç”±é™ä½æ•¸æ“šé‡ï¼Œå¸Œæœ›æ¨¡å‹ä¸è¦ train å¤ªä¹…ï¼Œ èƒ½ç›¡å¿«çœ‹åˆ°ä¸€å€‹æˆç¸¾ç•¶ä½œ baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# 1. ç¢ºèªè³‡æ–™æ ¼å¼\n",
    "print(\"æº–å‚™è³‡æ–™ä¸­...\")\n",
    "print(train_df_ttl.head())\n",
    "\n",
    "# 2. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_ttl['label'] = label_encoder.fit_transform(train_df_ttl['emotion'])  # æŠŠæƒ…æ„Ÿæ–‡å­—è½‰æ›ç‚ºæ•¸å€¼\n",
    "\n",
    "# 3. Train-Test Split\n",
    "train_df, val_df = train_test_split(train_df_ttl, test_size=0.2, stratify=train_df_ttl['label'], random_state=42)\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(train_df)}, é©—è­‰é›†å¤§å°: {len(val_df)}\")\n",
    "\n",
    "\n",
    "# 4. Under Sampling å‡½æ•¸\n",
    "def undersample_data(df, label_col):\n",
    "    min_count = df[label_col].value_counts().min()  # æœ€å°æ¨£æœ¬æ•¸\n",
    "    balanced_dfs = []\n",
    "    for label in df[label_col].unique():\n",
    "        class_subset = df[df[label_col] == label]\n",
    "        balanced_dfs.append(resample(class_subset, replace=False, n_samples=min_count, random_state=42))\n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    return balanced_df\n",
    "\n",
    "# é€²è¡Œ Under Sampling\n",
    "train_df_balanced = undersample_data(train_df, 'label')\n",
    "print(f\"å¹³è¡¡å¾Œçš„è¨“ç·´é›†å¤§å°: {len(train_df_balanced)}\")\n",
    "\n",
    "# 5. ä½¿ç”¨ DistilBERT çš„ tokenizer \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 6. å®šç¾©æ•¸æ“šé›†è™•ç†é¡åˆ¥\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        label = row['label']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# å»ºç«‹æ•¸æ“šé›†èˆ‡ DataLoader\n",
    "train_dataset = EmotionDataset(train_df_balanced, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 7. åˆå§‹åŒ–æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# 8. å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 9. è¨“ç·´å‡½æ•¸\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 10. é©—è­‰å‡½æ•¸ï¼ˆè¨ˆç®— Macro F1 Scoreï¼‰\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(dataloader), f1\n",
    "\n",
    "# 11. ä¸»è¨“ç·´è¿´åœˆ\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # åŸ·è¡Œè¨“ç·´\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    # åŸ·è¡Œé©—è­‰\n",
    "    val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    # è¼¸å‡ºè¨“ç·´èˆ‡é©—è­‰çµæœ\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Macro F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜æ¨¡å‹\n",
    "    epoch_dir = f'./distilbert_emotion_model_{epoch + 1}'\n",
    "    model.save_pretrained(epoch_dir)\n",
    "    tokenizer.save_pretrained(epoch_dir)\n",
    "    print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {epoch_dir}ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic2](img/pic2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¸½å…±è¨“ç·´äº†3å€‹ epochs,å› ç‚ºç¬¬3å€‹ epoch çš„è¡¨ç¾æœ€å¥½ï¼Œæ•…ç¹¼çºŒå¾€ä¸‹é€²è¡Œtest set é æ¸¬ï¼Œæ²’æœ‰é¡å¤–å†åˆ‡æ›æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é€²è¡Œ Test Set é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å®šç¾©æ¸¬è©¦æ•¸æ“šé›†è™•ç†é¡åˆ¥\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return item\n",
    "\n",
    "# 2. æ§‹å»ºæ¸¬è©¦ DataLoader\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 3. é æ¸¬å‡½æ•¸\n",
    "def predict_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Predicting\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# 4. å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬\n",
    "print(\"é–‹å§‹å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬...\")\n",
    "test_predictions = predict_model(model, test_loader, device)\n",
    "\n",
    "# 5. å°‡é æ¸¬çµæœè½‰æ›ç‚ºåŸå§‹æƒ…æ„Ÿæ¨™ç±¤\n",
    "test_df['predicted_emotion'] = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# 6. å„²å­˜é æ¸¬çµæœ\n",
    "'''\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']]  # åªä¿ç•™ tweet_id å’Œ emotion æ¬„ä½\n",
    "submission.to_csv('submission.csv', index=False)  # å„²å­˜ç‚º submission.csv\n",
    "print(\"æ¸¬è©¦æ•¸æ“šçš„é æ¸¬çµæœå·²ä¿å­˜ç‚º 'submission.csv'\")\n",
    "'''\n",
    "# é‡å‘½åæ¬„ä½\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']].rename(columns={\n",
    "    'tweet_id': 'id',\n",
    "    'predicted_emotion': 'emotion'\n",
    "})\n",
    "\n",
    "# å„²å­˜ç‚º submission.csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"æ¸¬è©¦æ•¸æ“šçš„é æ¸¬çµæœå·²ä¿å­˜ç‚º 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤é æ¸¬çµæœç²å¾— public score: 0.42491\n",
    "\n",
    "ç®—æ˜¯ä¸€å€‹ä¸­åä¸‹çš„æ’åèˆ‡æˆç¸¾\n",
    "\n",
    "å¯èƒ½æ˜¯å› ç‚ºundersampling è®“æ•¸æ“šé‡è®Šå¾—å¤ªå°ï¼Œå¯èƒ½æ¨¡å‹ä¸å¤ªå¥½å­¸ç¿’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic1](img/pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¦é©—2: \n",
    "\n",
    "ä¸åšsampling, ä½¿ç”¨å…¨éƒ¨çš„æ•¸æ“šå»è¨“ç·´\n",
    "\n",
    "ä½†ä¾èˆŠè€ƒé‡åˆ°æ•¸æ“šé‡ä¸å¹³å‡çš„ç‹€æ³ï¼Œæ•…å°‡æå¤±è©•ä¼°å¾ CrossEntropyLoss æ”¹æˆ focal lossï¼Œä»¥æœŸæ›´å¥½æ‡‰å°é¡åˆ¥ä¸å¹³è¡¡æƒ…æ³ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. ç¢ºèªè³‡æ–™æ ¼å¼\n",
    "print(\"æº–å‚™è³‡æ–™ä¸­...\")\n",
    "print(train_df_ttl.head())\n",
    "\n",
    "# Label Encoding æƒ…æ„Ÿæ¨™ç±¤\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_ttl['label'] = label_encoder.fit_transform(train_df_ttl['emotion'])  # æŠŠæƒ…æ„Ÿæ–‡å­—è½‰æ›ç‚ºæ•¸å€¼\n",
    "\n",
    "# 3. Train-Test Split\n",
    "train_df, val_df = train_test_split(train_df_ttl, test_size=0.2, stratify=train_df_ttl['label'], random_state=42)\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(train_df)}, é©—è­‰é›†å¤§å°: {len(val_df)}\")\n",
    "\n",
    "# 4. ä½¿ç”¨ DistilBERT çš„ tokenizer \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 5. å®šç¾©æ•¸æ“šé›†è™•ç†é¡åˆ¥\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        label = row['label']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "\n",
    "# 6. æ§‹å»º DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 7. åˆå§‹åŒ–æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# 8. å®šç¾© Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities for true labels\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss()  # ä½¿ç”¨ Focal Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 9. è¨“ç·´å‡½æ•¸\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 10. é©—è­‰å‡½æ•¸ï¼ˆä½¿ç”¨ F1 Scoreï¼‰\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    return f1\n",
    "\n",
    "# 11. ä¸»è¨“ç·´è¿´åœˆ\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # åŸ·è¡Œè¨“ç·´\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    # åŸ·è¡Œé©—è­‰\n",
    "    val_f1 = eval_model(model, val_loader, device)\n",
    "    \n",
    "    # è¼¸å‡ºè¨“ç·´èˆ‡é©—è­‰çµæœ\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜æ¨¡å‹\n",
    "    epoch_dir = f'./distilbert_emotion_model_{epoch + 1}'\n",
    "    model.save_pretrained(epoch_dir)\n",
    "    tokenizer.save_pretrained(epoch_dir)\n",
    "    print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {epoch_dir}ï¼\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic3](img/pic3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¨“ç·´ä¸­çš„ F1é©—è­‰æˆç¸¾å·²ç¶“çœ‹åˆ°æ¯”å¯¦é©—1é‚„è¦å¥½\n",
    "\n",
    "ä¸€æ¨£ï¼Œç¸½å…±è¨“ç·´äº†3å€‹ epochs,å› ç‚ºç¬¬3å€‹ epoch çš„è¡¨ç¾æœ€å¥½ï¼Œæ•…ç¹¼çºŒå¾€ä¸‹é€²è¡Œtest set é æ¸¬ï¼Œæ²’æœ‰é¡å¤–å†åˆ‡æ›æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åšTest Set é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å®šç¾©æ¸¬è©¦æ•¸æ“šé›†è™•ç†é¡åˆ¥\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['cleaned_text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return item\n",
    "\n",
    "# 2. æ§‹å»ºæ¸¬è©¦ DataLoader\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 3. é æ¸¬å‡½æ•¸\n",
    "def predict_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Predicting\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# 4. å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬\n",
    "print(\"é–‹å§‹å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬...\")\n",
    "test_predictions = predict_model(model, test_loader, device)\n",
    "\n",
    "# 5. å°‡é æ¸¬çµæœè½‰æ›ç‚ºåŸå§‹æƒ…æ„Ÿæ¨™ç±¤\n",
    "test_df['predicted_emotion'] = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# 6. å„²å­˜é æ¸¬çµæœ\n",
    "'''\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']]  # åªä¿ç•™ tweet_id å’Œ emotion æ¬„ä½\n",
    "submission.to_csv('submission.csv', index=False)  # å„²å­˜ç‚º submission.csv\n",
    "print(\"æ¸¬è©¦æ•¸æ“šçš„é æ¸¬çµæœå·²ä¿å­˜ç‚º 'submission.csv'\")\n",
    "'''\n",
    "# é‡å‘½åæ¬„ä½\n",
    "submission = test_df[['tweet_id', 'predicted_emotion']].rename(columns={\n",
    "    'tweet_id': 'id',\n",
    "    'predicted_emotion': 'emotion'\n",
    "})\n",
    "\n",
    "# å„²å­˜ç‚º submission.csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"æ¸¬è©¦æ•¸æ“šçš„é æ¸¬çµæœå·²ä¿å­˜ç‚º 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤é æ¸¬çµæœç²å¾— public score: 0.48862\n",
    "\n",
    "ç®—æ˜¯ä¸€å€‹ä¸­é–“çš„æ’åèˆ‡æˆç¸¾"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic4](img/pic4.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
